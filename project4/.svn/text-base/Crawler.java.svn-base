import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.OutputStreamWriter;
import java.io.PrintWriter;
import java.net.MalformedURLException;
import java.net.Socket;
import java.net.URL;
import java.net.UnknownHostException;
import java.util.HashSet;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

/**
 * This class connects to internet and cleans the HTML page and returns a simple
 * text
 * 
 * @author shreyarajani
 * 
 */
public class Crawler {

	private static final Logger logger = LogManager.getLogger();

	// TODO Use final where possible
	private MultithreadedInvertedIndex invertedIndex;
	private WorkQueue workers;
	private String baseurl;
	private Integer pending; // TODO This should be an int
	private HashSet<String> foundlinksMap;

	public static final String regex = "<[aA]\\s+.*?[hH][rR][eE][fF]\\s*=\\s*\"([^\"]+?)\"";

	/**
	 * Constructor to initialize the values
	 * 
	 * @param invertedIndex
	 *            - Index where words will be stored
	 * 
	 * @param workers
	 *            - workers that work to build the index
	 */
	public Crawler(MultithreadedInvertedIndex invertedIndex, WorkQueue workers) {
		this.invertedIndex = invertedIndex;
		this.workers = workers;
		pending = 0;
		foundlinksMap = new HashSet<String>();
	}

	/**
	 * This method parses line
	 * 
	 * @param line
	 *            - the line to be parsed
	 * @param other
	 *            - inverted index where the words will be stored
	 * @param url
	 *            - URL to be parsed
	 * @param position
	 *            - position of the word found in the HTML text
	 * @return
	 */
	private int parseLine(String line, MultithreadedInvertedIndex other,
			String url, int position) {
		int indexPosition = position;
		if (line.trim().length() != 0) {
			String[] wordsArray = line.split("\\s");
			for (String word : wordsArray) {
				word = word.trim().toLowerCase().replaceAll("\\W", "")
						.replaceAll("_", "");
				if (!word.isEmpty()) {
					indexPosition = indexPosition + 1;
					other.addEntry(word, url, indexPosition);
				}
			}
		}
		return indexPosition;
	}

	/**
	 * Strings the Elements
	 * 
	 * @param name
	 *            - takes the name
	 * @param html
	 *            - the sequence to be matched
	 * @return result
	 */
	public static String stripElement(String name, String html) {
		String pattern = "(?is)<" + name + "(.*?)>" + "(.*?)/" + name + "[ ]*>";
		Pattern p = Pattern.compile(pattern);
		Matcher m = p.matcher(html);
		String result = m.replaceAll("");
		return result;
	}

	/**
	 * Strips off the HTML tags
	 * 
	 * @param html
	 *            - the link from which tags have to be removed
	 * @return result
	 */
	public String stripTags(String html) {
		String pattern = "(?is)<.*?>";
		Pattern p = Pattern.compile(pattern);
		Matcher m = p.matcher(html);
		String result = m.replaceAll(" ");
		return result;
	}

	/**
	 * Strips the extra entities
	 * 
	 * @param html
	 *            - html to strip the entities
	 * @return result
	 */
	public static String stripEntities(String html) {
		String pattern = "(?is)&(.*?);";
		Pattern p = Pattern.compile(pattern);
		Matcher m = p.matcher(html);
		String result = m.replaceAll(" ");
		return result;
	}

	/**
	 * parses the words and workers are then called
	 * 
	 * @param url
	 *            - url from which the words are to be parsed
	 */
	public void parseWords(String url) {
		String pattern = "(?is)((.*)/)";
		Pattern p = Pattern.compile(pattern);
		Matcher m = p.matcher(url);
		if (m.find())
			baseurl = m.group(1);
		
		// TODO Everything above here shouldn't be necessary.
		foundlinksMap.add(url);
		workers.execute(new parseURL(url));
	}

	/**
	 * cleans the HTML
	 * 
	 * @param html
	 *            - html to be cleaned
	 * @return
	 */
	public String cleanHTML(String html) {
		String text = html;
		text = stripElement("script", text);
		text = stripElement("style", text);
		text = stripTags(text);
		text = stripEntities(text);
		return text;
	}

	/**
	 * finish method
	 */
	public synchronized void finish() {
		try {
			while (pending > 0) {
				// logger.debug("Waiting...");
				this.wait();
			}
		} catch (InterruptedException e) {
			// logger.debug("Finish interrupted", e);
		}
	}

	/**
	 * shutdown method
	 */
	public void shutdown() {
		// logger.debug("Shutting down now");
		finish();
		workers.shutdown();
	}

	/**
	 * increment pending method
	 */
	private synchronized void incrementPending() {
		pending++;
		// logger.debug("Pending is now {}", pending);
	}

	/**
	 * decrement pending method
	 */
	private synchronized void decrementPending() {
		pending--;
		// logger.debug("Pending is now {}", pending);
		if (pending <= 0) {
			this.notifyAll();
		}
	}

	/**
	 * Inner class to parse the URL
	 * 
	 * @author shreyarajani
	 * 
	 */
	private class parseURL implements Runnable { // TODO ParseURLWorker
		
		// TODO Use private and final where possible
		public int position;
		public URL link;
		private MultithreadedInvertedIndex privateInvertedIndex;
		public HashSet<String> privateLinksSet;

		/**
		 * Parses URL
		 * 
		 * @param url
		 *            - url to parse
		 */
		public parseURL(String url) {
			privateInvertedIndex = new MultithreadedInvertedIndex();
			privateLinksSet = new HashSet<String>();
			position = 0;

			incrementPending();
			try {
				link = new URL(url);
			} catch (Exception e) {
				System.out.println("Unable to parse URL :" + link);
			}
		}

		/**
		 * overrides the run method
		 */
		@Override
		public void run() {

			// TODO Use try-with-resources
			try {
				Socket socket = new Socket(link.getHost(), 80);

				PrintWriter writer = new PrintWriter(new OutputStreamWriter(
						socket.getOutputStream()));
				BufferedReader reader = new BufferedReader(
						new InputStreamReader(socket.getInputStream()));
				String request = "GET " + link.getFile() + " HTTP/1.1"
						+ "\nHost : " + link.getHost() + "\r\n\n";
				writer.print(request);
				writer.flush();

				String ignoreHeader = new String();
				boolean ignore = true;
				boolean checkHTML = false;

				while (ignore) {
					ignoreHeader = reader.readLine();
					if (ignoreHeader.contains("Content-Type: text/html")) {
						checkHTML = true;
					}

					if (ignoreHeader.isEmpty()) {
						ignore = false;
					}
				}

				if (checkHTML) {
					// TODO Look at StringBuffer instead of String
					String htmlFetch = "";
					boolean htmlStart = false;
					String inputLine;

					while ((inputLine = reader.readLine()) != null) {
						if (inputLine.toLowerCase().startsWith("<html")) {
							inputLine = reader.readLine();
							htmlStart = true;
						}
						if (htmlStart)
							htmlFetch = htmlFetch + " " + inputLine;
					}

					// TODO Use try-with-resources
					socket.close();
					reader.close();
					writer.close();

					privateLinksSet = recognizeLinks(htmlFetch);
					String cleaned = cleanHTML(htmlFetch);
					position = parseLine(cleaned, privateInvertedIndex,
							link.toString(), position);
					invertedIndex.addAll(privateInvertedIndex);
					
					// TODO Lock so you can add up to 50 links from private links set to your found links set
					// And create the workers here. 
					for (String str : privateLinksSet) {
						// Check if already have 50 links, or if already have the link, then stop
						// Otherwise add and create worker
						workers.execute(new parseURL(str));
					}
				}
				decrementPending();

			} catch (IOException e) {
				// TODO Output something
			}
		}

	}

	// TODO This method should maybe be private
	/**
	 * Crafts the HTTP get request from the URL
	 * 
	 * @param url
	 *            - for which the request has to be crafted
	 * @return
	 */
	protected String craftRequest(URL url) {
		String host = url.getHost();
		String resource = url.getFile().isEmpty() ? "/" : url.getFile();
		StringBuffer string = new StringBuffer();
		string.append("GET " + resource + " HTTP/1.1\n");
		string.append("Host: " + host + "\n");
		string.append("Connection: close\n");
		string.append("\r\n");
		return string.toString();
	}

	/**
	 * Stores the links that are recognized
	 * 
	 * @return the set of links
	 */
	private HashSet<String> recognizeLinks(String text)
			throws MalformedURLException {
		HashSet<String> linksSet = new HashSet<String>();
		Pattern p = Pattern.compile(regex);
		Matcher matcher = p.matcher(text);

		// TODO Simplify this loop considerably, do not worry about foundlinksMap here 
		while (matcher.find()) {
			// TODO Shouldn't be explicitly checking if link starts with http
			if (matcher.group(1).toLowerCase().startsWith("http")) {
				// TODO If the set already contains the link, it will not add
				if (!linksSet.contains(text)) {
					
					// TODO Unsafe reading and writing of shared data
					if (!foundlinksMap.contains(text)) {
						if (foundlinksMap.size() < 50) {
							linksSet.add(matcher.group(1));
							foundlinksMap.add(matcher.group(1));
						}
					}
				}

			} else if (!matcher.group(1).startsWith("#")) {
				URL base = new URL(baseurl);
				URL temp = new URL(base, matcher.group(1));

				if (!linksSet.contains(temp.toString())) {
					if (!foundlinksMap.contains(temp.toString())) {
						if (foundlinksMap.size() < 50) {
							linksSet.add(temp.toString());
							foundlinksMap.add(temp.toString());
						}
					}
				}
			}
		}
		return linksSet;
	}
}